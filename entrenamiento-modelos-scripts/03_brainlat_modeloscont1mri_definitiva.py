# -*- coding: utf-8 -*-
"""03_BrainLat_modelosConT1MRI_Definitiva.ipynb

Automatically generated by Colab.

# 1. LECTURA DE DATOS, DESCARGA Y VISUALIZACI√ìN.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade pysftp synapseclient python-dotenv
# %pip install --upgrade synapseclient
!pip install nibabel matplotlib numpy
!pip install pyradiomics SimpleITK
!pip uninstall -y tensorflow
!pip install tensorflow==2.12.0
!pip install monai
!pip install torch torchvision
!git clone https://github.com/Tencent/MedicalNet.git
!cd MedicalNet
!pip install -q monai torchio

import os
import pandas as pd
from dotenv import load_dotenv
import synapseclient
import synapseutils
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from scipy.ndimage import zoom
from google.colab import drive
drive.mount('/content/drive')

# Leemos el CSV con los pacientes
filtered_df = pd.read_csv("directorio_censurado/BrainLat_Imputado3.csv", delimiter=";")
patient_ids = filtered_df["MRI_ID"].unique()

import os
import pandas as pd
from dotenv import load_dotenv
import synapseclient
import synapseutils

load_dotenv()
# Iniciamos sesi√≥n en Synapse
SYNAPSE_TOKEN = "token_censurado"
syn = synapseclient.Synapse()
syn.login(authToken=SYNAPSE_TOKEN)

# Leemos el CSV con los pacientes
filtered_df = pd.read_csv("directorio_censurado/BrainLat_Imputado3.csv", delimiter=";")
patient_ids = filtered_df["MRI_ID"].unique()

# Obtenemos la lista de archivos disponibles en Synapse
entity = syn.get('syn51549340', downloadFile=False)
file_list = list(synapseutils.walk(syn, entity.id))

# Descargamos solo los archivos que coincidan con los pacientes en MRI_ID
for dirpath, dirnames, filenames in file_list:
  for dir in dirpath:
    for filename in filenames:
      for patient_id in patient_ids:
        if patient_id in dir and "BrainLat_dataset_NewIDs" in dir:
          file_path = os.path.join('/Volumes/T7/BrainLat', dir[17:])
          try:
            syn.get(filename[1], downloadLocation=file_path)
            print(f"‚úÖ Descargado: {filename[0]}")
          except Exception as e:
            print(f"‚ùå Error con {filename[0]}: {e}")

import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt

# Ruta de la imagen
nii_path = "/Volumes/T7/BrainLat/MRI data/BrainLat_dataset_NewIDs/sub-COB086/anat/sub-COB086_T1w.nii.gz"

img = nib.load(nii_path)

# Convertimos a array NumPy
data = img.get_fdata()

# Mostramos informaci√≥n
print(f"Dimensiones de la imagen: {data.shape}")
print(f"Tipo de datos: {data.dtype}")

data_3d = data[:, :, :]  # Extraemos el volumen 3D

slice_x = data_3d.shape[0] // 2  # Corte Sagital
slice_y = data_3d.shape[1] // 2  # Corte Coronal
slice_z = data_3d.shape[2] // 2  # Corte Axial

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Corte Coronal (Vista frontal)
axes[0].imshow(data_3d[slice_x, :, :], cmap="gray", origin="lower")
axes[0].set_title("Corte Coronal (Frontal)")

# Corte Axial (Vista superior/inferior)
axes[1].imshow(data_3d[:, slice_y, :], cmap="gray", origin="lower")
axes[1].set_title("Corte Axial (Superior)")

# Corte Sagital (Vista lateral)
axes[2].imshow(data_3d[:, :, slice_z], cmap="gray", origin="lower")
axes[2].set_title("Corte Sagital (Lateral)")

for ax in axes:
    ax.axis("off")

plt.show()

# Mostramos misma imagen pero con colores
import matplotlib.pyplot as plt
import nibabel as nib

slice_x = data_3d.shape[0] // 2  # Corte Sagital
slice_y = data_3d.shape[1] // 2  # Corte Coronal
slice_z = data_3d.shape[2] // 2  # Corte Axial

fig, axes = plt.subplots(1, 3, figsize=(15, 5))
colormap = "jet"

# Corte Coronal (Vista frontal)
axes[0].imshow(data_3d[slice_x, :, :], cmap=colormap, origin="lower")
axes[0].set_title("Corte Coronal (Frontal)")

# Corte Axial (Vista superior/inferior)
axes[1].imshow(data_3d[:, slice_y, :], cmap=colormap, origin="lower")
axes[1].set_title("Corte Axial (Superior)")

# Corte Sagital (Vista lateral)
axes[2].imshow(data_3d[:, :, slice_z], cmap=colormap, origin="lower")
axes[2].set_title("Corte Sagital (Lateral)")

for ax in axes:
    ax.axis("off")

plt.show()

# Tanto data como data_3d son matrices de datos num√©ricos y el slice es la forma original de ese campo dividido entre 2
print(slice_z)
print(data_3d[slice_x, :, :])

"""# 2. PREPARACI√ìN CONJUNTO DE DATOS.

"""

import nibabel as nib
import numpy as np
import pandas as pd
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from scipy.ndimage import zoom

base_path = "/Volumes/T7/BrainLat/MRI data/BrainLat_dataset_NewIDs"

# Funci√≥n para verificar si existe la imagen de 'T1'
def imagen_existente(mri_id):
    image_path = os.path.join(base_path, mri_id, 'anat', f"{mri_id}_T1w.nii.gz")
    return os.path.exists(image_path)

filtered_df["has_image"] = filtered_df["MRI_ID"].apply(imagen_existente)

# Nos quedamos solo con los que s√≠ tienen imagen
filtered_df_with_image = filtered_df[filtered_df["has_image"] == True].copy()

label2id = {
    "AD": 0,
    "CN": 1,
    "PD": 2,
    "FTD": 3
}

TARGET_SHAPE = (64, 64, 40)

X = []
y = []
mri_ids = []

for i, row in filtered_df_with_image.iterrows():
    try:
        img_path = f"/Volumes/T7/BrainLat/MRI data/BrainLat_dataset_NewIDs/{row['MRI_ID']}/anat/{row['MRI_ID']}_T1w.nii.gz"
        img = nib.load(img_path)
        data = img.get_fdata()
        data_3d = data[:, :, :]
        # Para ocupar menos lugar
        data_3d = data_3d.astype(np.float32)

        # Normalizamos
        data_3d = (data_3d - np.mean(data_3d)) / np.std(data_3d)

        zoom_factors = (
            TARGET_SHAPE[0] / data_3d.shape[0],
            TARGET_SHAPE[1] / data_3d.shape[1],
            TARGET_SHAPE[2] / data_3d.shape[2]
        )

        # Redimensionamos el volumen
        data_3d_resized = zoom(data_3d, zoom_factors)

        # Expandimos el canal
        data_3d_resized = np.expand_dims(data_3d_resized, axis=-1)

        X.append(data_3d_resized)
        y.append(label2id[row['diagnosis']])
        mri_ids.append(row['MRI_ID'])  # A√±adimos solo si se pudo procesar

    except Exception as e:
        print(f"Error procesando {row['MRI_ID']} ({row['diagnosis']}): {e}")

X = np.array(X)
y = np.array(y, dtype=np.int32)
mri_ids = np.array(mri_ids)
print(X.shape)  # Se muestra: (n_samples, 64, 64, 40, 1)
print(y.shape)

# Mostramos X para el individuo 1 con forma (64 para eje X, 64 para eje Y, 40 para eje X), su imagen es un cubo 3D con todo valores num√©ricos
print(X[1])

# Examinamos las clases que restan para im√°genes tipo T1
conteo_diagnosticos = filtered_df_with_image['diagnosis'].value_counts()

print("Recuento de diagn√≥sticos:")
print(conteo_diagnosticos)

"""# 3. PREPARACI√ìN DE ETIQUETAS."""

# Codificamos etiquetas
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

# Ahora AD:0, CN:1 y FTD:2
print(y_encoded)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test, train_ids, test_ids = train_test_split(X, y_encoded, mri_ids, test_size=0.2, stratify=y_encoded, random_state=42)

# Guardado
#from google.colab import drive
#drive.mount('/content/drive')

#np.savez_compressed("/content/drive/MyDrive/TFG/train_data.npz", X=X_train, y=y_train, ids=train_ids)
#np.savez_compressed("/content/drive/MyDrive/TFG/test_data.npz", X=X_test, y=y_test, ids=test_ids)

# Carga
# from google.colab import drive
# drive.mount('/content/drive')

train = np.load("/content/drive/MyDrive/TFG/train_data.npz", allow_pickle=True)
X_train = train["X"]
y_train = train["y"]
train_ids = train["ids"].astype(str)  # <-- 100% alineado: X_train[i] corresponde a y_train[i] y a train_ids[i], mismo orden

test = np.load("/content/drive/MyDrive/TFG/test_data.npz", allow_pickle=True)
X_test = test["X"]
y_test = test["y"]
test_ids = test["ids"].astype(str)

print(train)

# Carga y divisi√≥n variables demogr√°ficas
import pandas as pd
import numpy as np

df = pd.read_csv("/content/drive/MyDrive/TFG/BrainLat_Imputado3.csv", delimiter=";")
df["MRI_ID"] = df["MRI_ID"].astype(str)

# Filtramos √∫nicamente los MRI_IDs presentes en los datos de imagen
filtered_df = df[df["MRI_ID"].isin(np.concatenate([train_ids, test_ids]))].copy()

# Dividimos en conjuntos train y test manteniendo el orden
df_train = filtered_df.set_index("MRI_ID").loc[train_ids].reset_index()
df_test = filtered_df.set_index("MRI_ID").loc[test_ids].reset_index()

df_train.shape, df_test.shape

df_train.head()

"""# 4. SELECCI√ìN DE EMBEDDINGS PARA MRIs T1.

## 4.1. TIPO A: CNN Feature Extractor.
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, Flatten, Dense

input_layer = Input(shape=(64, 64, 40, 1))
x = Conv3D(16, (3, 3, 3), activation='relu', padding='same')(input_layer)
x = MaxPooling3D((2, 2, 2))(x)
x = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(x) # Conv3D + MaxPooling3D: buena combinaci√≥n para datos volum√©tricos como resonancias T1
x = MaxPooling3D((2, 2, 2))(x)
x = Flatten()(x)
x = Dense(128, activation='relu')(x)  # Este ser√° nuestro embedding

# No sobreajustamos con redes gigantes innecesarias en datasets m√©dicos peque√±os
# Modelo extractor
cnn_model1 = Model(inputs=input_layer, outputs=x)

# Versi√≥n con posibles mejoras
from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, GlobalAveragePooling3D, Dense, BatchNormalization, Activation
from tensorflow.keras.models import Model

input_layer = Input(shape=(64, 64, 40, 1))
x = Conv3D(32, (3, 3, 3), padding='same')(input_layer)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling3D((2, 2, 2))(x)

x = Conv3D(64, (3, 3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling3D((2, 2, 2))(x)

x = GlobalAveragePooling3D()(x)
embedding = Dense(128, activation='relu')(x)

cnn_model2 = Model(inputs=input_layer, outputs=embedding)

embeddings_cnn1 = cnn_model1.predict(X_train, batch_size=16)
embeddings_cnn2 = cnn_model2.predict(X_train, batch_size=16)

"""## 4.2. TIPO B: PCA."""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# Aplanamos primero: (n_samples, 64*64*40)
X_flat = X_train.reshape((X_train.shape[0], -1))

# Aplicamos PCA sin limitar componentes
pca_full = PCA().fit(X_flat)

# Varianza acumulada
cum_var_exp = np.cumsum(pca_full.explained_variance_ratio_)

# Elegimos el n√∫mero de componentes que explica al menos el 95% de la varianza
n_components_optimo = np.argmax(cum_var_exp >= 0.95) + 1

# Graficamos
plt.figure(figsize=(8, 4))
plt.plot(cum_var_exp, marker='o')
plt.axhline(0.95, color='r', linestyle='--')
plt.axvline(n_components_optimo, color='g', linestyle='--')
plt.title("Varianza explicada acumulada")
plt.xlabel("N√∫mero de componentes")
plt.ylabel("Varianza acumulada")
plt.grid(True)
plt.show()

print(f"N√∫mero √≥ptimo de componentes (‚â•95% varianza): {n_components_optimo}")

pca = PCA(n_components=207)
embeddings_pca = pca.fit_transform(X_flat)

"""## 4.3. TIPO C: Autoencoder."""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D

input_layer = Input(shape=(64, 64, 40, 1))
x = Conv3D(16, (3, 3, 3), activation='relu', padding='same')(input_layer)
x = MaxPooling3D((2, 2, 2), padding='same')(x)
x = Conv3D(8, (3, 3, 3), activation='relu', padding='same')(x)
encoded = MaxPooling3D((2, 2, 2), padding='same')(x)

# Decoder
x = Conv3D(8, (3, 3, 3), activation='relu', padding='same')(encoded)
x = UpSampling3D((2, 2, 2))(x)
x = Conv3D(16, (3, 3, 3), activation='relu')(x)
x = UpSampling3D((2, 2, 2))(x)
decoded = Conv3D(1, (3, 3, 3), activation='sigmoid', padding='same')(x)

autoencoder = Model(input_layer, decoded)
encoder_model = Model(inputs=input_layer, outputs=encoded)

embeddings_ae = encoder_model.predict(X_train)
embeddings_ae_flat = embeddings_ae.reshape((X_train.shape[0], -1))

"""## 4.4. Aspecto embeddings."""

print("Aspecto embedding con CNN: ", embeddings_cnn1)
print("Aspecto embedding con PCA: ", embeddings_pca)
print("Aspecto embedding con Autoencoder flat: ", embeddings_ae_flat)

"""# 5. ENTRENAMIENTO DE MODELOS.

## 5.1. BLOQUE A: Tests cognitivos + variables demogr√°ficas.
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neural_network import MLPClassifier
from sklearn.cross_decomposition import PLSRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.base import BaseEstimator, ClassifierMixin

# CLASE ENVOLTORIO PARA PLS-DA (PLS + LDA)
class PLS_DA(BaseEstimator, ClassifierMixin):
    def __init__(self, n_components=2):
        self.n_components = n_components
        self.pls = PLSRegression(n_components=n_components)
        self.clf = LinearDiscriminantAnalysis()

    def fit(self, X, y):
        self.pls.fit(X, y)
        X_pls = self.pls.transform(X)
        self.clf.fit(X_pls, y)
        return self

    def predict(self, X):
        X_pls = self.pls.transform(X)
        return self.clf.predict(X_pls)

# 1. Variables del Bloque A
demograficas = ["sex", "Age", "years_education", "laterality", "country", "center"]
cognitivos = [
    "ifs_total_score", "ifs_motor_series", "ifs_conflicting_instructions",
    "ifs_motor_inhibition", "ifs_digits", "ifs_months", "ifs_visual_wm",
    "ifs_proverb", "ifs_verbal_inhibition", "mini_sea_fer", "mini_sea_tom"
]
df_cols = demograficas + cognitivos

# 2. Selecci√≥n columnas
df_y_train = df_train["diagnosis"].map({
    "AD": 0,
    "CN": 1,
    "FTD": 2
})
df_y_test = df_test["diagnosis"].map({
    "AD": 0,
    "CN": 1,
    "FTD": 2
})
df_train_bloqueA = df_train[df_cols]
df_test_bloqueA = df_test[df_cols]

# 3. Preprocesamiento
categorical_cols = ["sex", "laterality", "country", "center"]
numerical_cols = ["Age", "years_education"] + cognitivos

preprocessor = ColumnTransformer([
    ("num", StandardScaler(), numerical_cols),
    ("cat", OneHotEncoder(drop='first'), categorical_cols)
])

# 4. Modelos
modelos = {
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "SVM": SVC(kernel='linear', probability=True),
    "LDA": LinearDiscriminantAnalysis(),
    "MLP (NN)": MLPClassifier(hidden_layer_sizes=(64,), max_iter=500, random_state=42),
    "PLS-DA": PLS_DA(n_components=4)
}

# 5. Entrenamiento y evaluaci√≥n
for nombre, modelo in modelos.items():
    # Aplicamos oversampling con smote para el balanceo de datos
    pipeline = Pipeline([
        ("preproc", preprocessor),
        ("smote", SMOTE(random_state=42)),
        ("clf", modelo)
    ])
    pipeline.fit(df_train_bloqueA, df_y_train)
    y_pred = pipeline.predict(df_test_bloqueA)

    acc = accuracy_score(df_y_test, y_pred)
    f1 = f1_score(df_y_test, y_pred, average="weighted")

    print(f"\n=== {nombre} ===")
    print(f"Accuracy: {acc:.4f}")
    print(f"F1-score: {f1:.4f}")
    print(classification_report(df_y_test, y_pred))

import pandas as pd

# Tabla con resultados bloque A
data = {
    "Modelo": ["Random Forest", "SVM", "LDA", "MLP (NN)", "PLS-DA"],
    "Accuracy": [0.8361, 0.8197, 0.8033, 0.8197, 0.7869],
    "F1-score": [0.8296, 0.8185, 0.7983, 0.8165, 0.7857]
}

tabla_resumen = pd.DataFrame(data)

print("=== Tabla resumen de desempe√±o por modelo bloque A ===")
print(tabla_resumen.to_string(index=False))

"""## 5.2. BLOQUE B: Embeddings A, B y C.

"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neural_network import MLPClassifier
from sklearn.cross_decomposition import PLSRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.base import BaseEstimator, ClassifierMixin

# CLASE PARA PLS-DA
class PLS_DA(BaseEstimator, ClassifierMixin):
    def __init__(self, n_components=5):
        self.n_components = n_components
        self.pls = PLSRegression(n_components=n_components)
        self.clf = LinearDiscriminantAnalysis()

    def fit(self, X, y):
        self.pls.fit(X, y)
        X_pls = self.pls.transform(X)
        self.clf.fit(X_pls, y)
        return self

    def predict(self, X):
        X_pls = self.pls.transform(X)
        return self.clf.predict(X_pls)

# 1. Carga de datos
train = np.load("/content/drive/MyDrive/TFG/train_data.npz", allow_pickle=True)
X_train = train["X"]
y_train = train["y"]
train_ids = train["ids"]

test = np.load("/content/drive/MyDrive/TFG/test_data.npz", allow_pickle=True)
X_test = test["X"]
y_test = test["y"]
test_ids = test["ids"]

embeddings_cnn1_train = embeddings_cnn1
embeddings_cnn2_train = embeddings_cnn2
embeddings_cnn1_test = cnn_model1.predict(X_test, batch_size=16)
embeddings_cnn2_test = cnn_model2.predict(X_test, batch_size=16)

embeddings_pca_train = embeddings_pca
X_test_flat = X_test.reshape((X_test.shape[0], -1))
embeddings_pca_test = pca.transform(X_test_flat)

embeddings_ae_flat_train = embeddings_ae_flat
embeddings_ae_test = encoder_model.predict(X_test)
embeddings_ae_flat_test = embeddings_ae_test.reshape((X_test.shape[0], -1))

# 2. Escalado global
scaler = StandardScaler()
X_trainA1 = scaler.fit_transform(embeddings_cnn1_train)
X_testA1 = scaler.transform(embeddings_cnn1_test)
scaler2 = StandardScaler()
X_trainA2 = scaler2.fit_transform(embeddings_cnn2_train)
X_testA2 = scaler2.transform(embeddings_cnn2_test)

scaler3 = StandardScaler()
X_trainB = scaler3.fit_transform(embeddings_pca_train)
X_testB = scaler3.transform(embeddings_pca_test)

scaler4 = StandardScaler()
X_trainC = scaler4.fit_transform(embeddings_ae_flat_train)
X_testC = scaler4.transform(embeddings_ae_flat_test)

# 3. Modelos a comparar
modelos = {
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "SVM": SVC(kernel='linear', probability=True),
    "LDA": LinearDiscriminantAnalysis(),
    "MLP (NN)": MLPClassifier(hidden_layer_sizes=(64,), max_iter=500, random_state=42),
    "PLS-DA": PLS_DA(n_components=5)
}

# 4. Entrenamiento y evaluaci√≥n
embeddings_datasets = {
    "CNN1": (X_trainA1, X_testA1),
    "CNN2": (X_trainA2, X_testA2),
    "PCA": (X_trainB, X_testB),
    "Autoencoder": (X_trainC, X_testC)
}
for emb,conj in embeddings_datasets.items():
  print(f"{emb}")
  # Aplicamos oversampling solo a las muestras de entrenamiento
  smote = SMOTE(random_state=42)
  X_train_balanced, y_train_balanced = smote.fit_resample(conj[0], y_train)
  for nombre, modelo in modelos.items():
      modelo.fit(X_train_balanced, y_train_balanced)
      y_pred = modelo.predict(conj[1])

      acc = accuracy_score(y_test, y_pred)
      f1 = f1_score(y_test, y_pred, average="weighted")

      print(f"\n=== {nombre} ===")
      print(f"Accuracy: {acc:.4f}")
      print(f"F1-score: {f1:.4f}")
      print(classification_report(y_test, y_pred))

import pandas as pd

# Tabla con los mejores resultados
data = {
    "Embedding": ["CNN1", "CNN2", "PCA", "Autoencoder"],
    "Mejor Modelo": ["PLS-DA", "MLP (NN)", "SVM", "Random Forest"],
    "Accuracy": [0.7049, 0.7049, 0.7213, 0.7541],
    "F1-score": [0.7030, 0.7031, 0.7153, 0.7427]
}

tabla_resumen = pd.DataFrame(data)

print("=== Tabla resumen de desempe√±o por embedding ===")
print(tabla_resumen.to_string(index=False))

"""El mejor embedding parece ser Autoencoder en principio, seguido de cerca por PCA y luego CNN2, porque:

- Autoencoder es una buena opci√≥n si quieres preservar una representaci√≥n no lineal, aunque ligeramente inferior.

- PCA es muy competitivo y puede ser preferido si se busca interpretabilidad.

- CNN2 con MLP ofrece el mejor equilibrio entre clases, mayor robustez y F1 m√°s alto.
"""

# Aplicamos cross validation porque resulta sospechoso que el mejor embedding sea el obtenido con Autoencoder siendo que:
# - Las redes neuronales convolucionales son las mejores para sacar features de im√°genes.
# - CNN2 es una red convolucional con mejoras introducidas.
import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler

# Escalamos los embeddings
scaler = StandardScaler()
cnn1 = scaler.fit_transform(embeddings_cnn1_train)
cnn2 = scaler.fit_transform(embeddings_cnn2_train)
pca  = scaler.fit_transform(embeddings_pca_train)
ae   = scaler.fit_transform(embeddings_ae_flat_train)

# Diccionario con mejores modelos por embedding
mejores_modelos = {
    "CNN1": (cnn1, RandomForestClassifier(n_estimators=100, random_state=42)),
    "CNN2": (cnn2, RandomForestClassifier(n_estimators=100, random_state=42)),
    "PCA":  (pca,  SVC(kernel='linear')),
    "Autoencoder": (ae, RandomForestClassifier(n_estimators=100, random_state=42))
}

# Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

resultados_cv = []
for nombre, (X, modelo) in mejores_modelos.items():
    acc_scores = cross_val_score(modelo, X, y_train, cv=cv, scoring='accuracy')
    f1_scores = cross_val_score(modelo, X, y_train, cv=cv, scoring='f1_weighted')

    resultados_cv.append({
        "Embedding": nombre,
        "Mejor Modelo": type(modelo).__name__,
        "Accuracy (CV)": np.mean(acc_scores),
        "F1-score (CV)": np.mean(f1_scores)
    })

# Creamos DataFrame resumen
tabla_cv = pd.DataFrame(resultados_cv)
print("=== Resultados con Validaci√≥n Cruzada (5-Fold) ===")
print(tabla_cv.to_string(index=False))

"""Finalmente, la mejor opci√≥n para obtener embeddings ser√≠a CNN con diferencia con Random Forest. Se har√° uso de este embedding en el entrenamiento de los datos que constituyen el bloque C.

## 5.3. BLOQUE C: Tests cognitivos + variables demogr√°ficas + mejor embedding.
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# 1. Datos preparados bloque A
# df_train_bloqueA
# df_test_bloqueA

# 2. Preprocesamiento
preprocessor = ColumnTransformer([
    ("num", StandardScaler(), numerical_cols),
    ("cat", OneHotEncoder(drop='first'), categorical_cols)
])

df_trainA_proc = preprocessor.fit_transform(df_train_bloqueA)
df_testA_proc = preprocessor.transform(df_test_bloqueA)

# 3. Concatenaci√≥n
X_train_concat = np.concatenate([df_trainA_proc, embeddings_cnn2_train], axis=1)
X_test_concat = np.concatenate([df_testA_proc, embeddings_cnn2_test], axis=1)

print(df_trainA_proc[1]) # 13 num√©ricas escaladas, 2 binarias y 2 categ√≥ricas a las que se les aplica el one-hot encoding

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neural_network import MLPClassifier
from sklearn.cross_decomposition import PLSRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, ClassifierMixin
from imblearn.over_sampling import SMOTE

# Clase para PLS-DA
class PLS_DA(BaseEstimator, ClassifierMixin):
    def __init__(self, n_components=5):
        self.n_components = n_components
        self.pls = PLSRegression(n_components=n_components)
        self.clf = LinearDiscriminantAnalysis()

    def fit(self, X, y):
        self.pls.fit(X, y)
        X_pls = self.pls.transform(X)
        self.clf.fit(X_pls, y)
        return self

    def predict(self, X):
        X_pls = self.pls.transform(X)
        return self.clf.predict(X_pls)

# Normalizaci√≥n de los datos combinados
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_concat)
X_test_scaled = scaler.transform(X_test_concat)

# Modelos a evaluar
modelos = {
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "SVM (Linear)": SVC(kernel='linear', probability=True),
    "LDA": LinearDiscriminantAnalysis(),
    "MLP (NN)": MLPClassifier(hidden_layer_sizes=(64,), max_iter=500, random_state=42),
    "PLS-DA": PLS_DA(n_components=5)
}

# Entrenamiento y evaluaci√≥n
resultados = []

# Aplicamos oversampling solo a las muestras de entrenamiento
smote = SMOTE(random_state=42)
X_train_scaled_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

for nombre, modelo in modelos.items():
    modelo.fit(X_train_scaled_balanced, y_train_balanced)
    y_pred = modelo.predict(X_test_scaled)

    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average="weighted")

    print(f"\n=== {nombre} ===")
    print(f"Accuracy: {acc:.4f}")
    print(f"F1-score: {f1:.4f}")
    print(classification_report(y_test, y_pred))

    resultados.append({
        "Modelo": nombre,
        "Accuracy": acc,
        "F1-score": f1
    })

# Mostrar resultados en tabla
df_resultados = pd.DataFrame(resultados)
print("\nResumen comparativo:")
print(df_resultados)

"""¬øPor qu√© ha funcionado tan bien el Random Forest?

## 5.4. CNN2: Clasificaci√≥n MRIs ponderadas en T1.
"""

import numpy as np
from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, GlobalAveragePooling3D, Dense, BatchNormalization, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, accuracy_score, f1_score
from tensorflow.keras.callbacks import EarlyStopping

# Carga de datos
train = np.load("/content/drive/MyDrive/TFG/train_data.npz", allow_pickle=True)
X_train = train["X"]
y_train = train["y"]
train_ids = train["ids"].astype(str)

test = np.load("/content/drive/MyDrive/TFG/test_data.npz", allow_pickle=True)
X_test = test["X"]
y_test = test["y"]
test_ids = test["ids"].astype(str)

# Aseguramos forma y tipo
X_train = X_train.astype(np.float32)
X_test = X_test.astype(np.float32)
y_train_cat = to_categorical(y_train, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

# Red para clasificaci√≥n (se trata de la CNN2)
input_layer = Input(shape=(64, 64, 40, 1))
x = Conv3D(32, (3, 3, 3), padding='same')(input_layer)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling3D((2, 2, 2))(x)

x = Conv3D(64, (3, 3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling3D((2, 2, 2))(x)

x = GlobalAveragePooling3D()(x)
x = Dense(128, activation='relu')(x)
output_layer = Dense(3, activation='softmax')(x)

cnn_classifier = Model(inputs=input_layer, outputs=output_layer)
cnn_classifier.compile(optimizer=Adam(learning_rate=0.001),
                       loss='categorical_crossentropy',
                       metrics=['accuracy'])

# Entrenamiento
early_stop = EarlyStopping(patience=10, restore_best_weights=True)
cnn_classifier.fit(X_train, y_train_cat,
                   validation_data=(X_test, y_test_cat),
                   epochs=50,
                   batch_size=16,
                   callbacks=[early_stop],
                   verbose=2)

# Evaluaci√≥n
y_pred_probs = cnn_classifier.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)
acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average="weighted")
report = classification_report(y_test, y_pred, digits=4)

(acc, f1, report)

# Resultados clasificaci√≥n MRIs con CNN2
import pandas as pd

data = {
    "Clase": ["AD", "CN", "FTD", "Global"],
    "Precision": [0.6053, 0.5385, 0.6000, 0.5810],
    "Recall": [0.9200, 0.3333, 0.4000, 0.5902],
    "F1-score": [0.7302, 0.4118, 0.4800, 0.5590],
    "Support": [25, 21, 15, 61]
}

tabla_metrica_clase = pd.DataFrame(data)

print("=== M√©tricas por clase para CNN2 ===")
print(tabla_metrica_clase.to_string(index=False))

"""- Tendencia hacia AD: el clasificador est√° muy ajustado para identificar AD (puede ser debido a la mayor cantidad de AD en los datos o por patrones m√°s claros).

- Desbalance y superposici√≥n de clases: CN y FTD suelen cruzarse en el espacio latente que ha sido aprendido por el modelo, complicando su diferenciaci√≥n precisa.

- Interpretaci√≥n cl√≠nica: podr√≠amos se√±alar que el modelo es eficaz para identificar AD, pero debe perfeccionarse en la diferenciaci√≥n de casos sanos o FTD para prevenir falsos negativos

El rendimiento global de la clasificaci√≥n directa de las im√°genes de resonancia magn√©tica con CNN2 no resulta satisfactorio, por lo que se obvia esta opci√≥n y se procede con los embeddings + tests cognitivos + variables demogr√°ficas.

# 6. INTERPRETACI√ìN DEL MEJOR MODELO APLICADO AL BLOQUE C: Random Forest.
"""

import numpy as np
import pandas as pd
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

# 1. Normalizaci√≥n
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_concat)
X_test_scaled = scaler.transform(X_test_concat)

# 2. Balanceo del conjunto de entrenamiento
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

# 3. Definici√≥n y entrenamiento del modelo
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_balanced, y_train_balanced)

# 4. Evaluaci√≥n
y_pred = rf_model.predict(X_test_scaled)
acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average="weighted")

# 5. Resultados
print("\n=== Random Forest ===")
print(f"Accuracy: {acc:.4f}")
print(f"F1-score: {f1:.4f}")
print(classification_report(y_test, y_pred))

# 6. Tabla resumen
df_resultado = pd.DataFrame([{
    "Modelo": "Random Forest",
    "Accuracy": acc,
    "F1-score": f1
}])
print("\nResumen:")
print(df_resultado)

# Matriz de confusi√≥n
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['AD', 'CN', 'FTD'],
            yticklabels=['AD', 'CN', 'FTD'])
plt.xlabel('Predicho')
plt.ylabel('Real')
plt.title('Matriz de Confusi√≥n')
plt.show()

# Reporte clasificaci√≥n por clase
from sklearn.metrics import classification_report
import pandas as pd

report = classification_report(y_test, y_pred, output_dict=True, target_names=['AD', 'CN', 'FTD'])
df_report = pd.DataFrame(report).transpose().drop('accuracy', errors='ignore')

df_report[['precision', 'recall', 'f1-score']].plot(kind='bar', figsize=(10, 6))
plt.title('M√©tricas por Clase')
plt.ylim(0, 1)
plt.legend(loc='lower right')
plt.show()

# Curvas ROC por clase
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
from itertools import cycle

# Binarizamos las etiquetas
y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
y_pred_prob = rf_model.predict_proba(X_test_scaled)

# Calculamos ROC para cada clase
fpr = dict()
tpr = dict()
roc_auc = dict()
n_classes = 3

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot
plt.figure(figsize=(8, 6))
colors = cycle(['blue', 'red', 'green'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'Clase {i} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Curva ROC Multiclase')
plt.legend(loc='lower right')
plt.show()

# Importancia de variables
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]

# Nombres de caracter√≠sticas
feature_names = numerical_cols + list(preprocessor.named_transformers_['cat'].get_feature_names_out()) + \
                [f'embedding_{i}' for i in range(embeddings_cnn2_train.shape[1])]

plt.figure(figsize=(12, 8))
plt.title('Importancia de Variables')
plt.bar(range(30), importances[indices[:30]], align='center', color='skyblue')  # Top 20
plt.xticks(range(30), [feature_names[i] for i in indices[:30]], rotation=90)
plt.tight_layout()
plt.show()

# Distribuci√≥n de errores
error_df = pd.DataFrame({
    'Real': y_test,
    'Predicho': y_pred,
    'Correcto': y_test == y_pred
})

sns.countplot(data=error_df, x='Real', hue='Correcto',
              palette={True: 'skyblue', False: 'red'})
plt.xticks([0, 1, 2], ['AD', 'CN', 'FTD'])
plt.title('Aciertos vs Errores por Clase')
plt.show()

# An√°lisis clases mal clasificadas
misclassified = error_df[error_df['Correcto'] == False]
sns.heatmap(pd.crosstab(misclassified['Real'], misclassified['Predicho']),
            annot=True, fmt='d', cmap='Reds')
plt.xlabel('Predicho')
plt.ylabel('Real')
plt.title('Errores entre Clases')
plt.show()

# Representaci√≥n de mapas de calor con importancia de los embeddings
importances = rf_model.feature_importances_

# Los embeddings est√°n al final de X_train_concat
embedding_importances = importances[-128:]  # 128 es el tama√±o de los embeddings

# Gr√°fico de importancia de los embeddings
plt.figure(figsize=(12, 6))
plt.bar(range(128), embedding_importances, color='skyblue')
plt.xlabel('Dimensi√≥n del Embedding')
plt.ylabel('Importancia en el Modelo')
plt.title('Importancia de cada Dimensi√≥n del Embedding')
plt.show()

from tensorflow.keras.models import Model

dense_layer = cnn_model2.layers[-1]  # Capa Dense(128)
dense_weights = dense_layer.get_weights()[0]  # Pesos shape: (64, 128)

# Combinamos importancias de los embeddings con los pesos de la capa Dense
channel_importance = np.dot(dense_weights, embedding_importances)  # Shape: (64,)

# Diccionario para activaciones ponderadas por grupo
group_heatmaps = {0: np.zeros((32, 32, 20)),  # AD
                  1: np.zeros((32, 32, 20)),  # CN
                  2: np.zeros((32, 32, 20))}  # FTD

for i in range(len(X_train)):
    diagnosis = y_train[i]
    conv_act, _ = activation_model.predict(X_train[i:i+1])  # Shape: (1, 32, 32, 20, 64)
    conv_act = conv_act[0]  # Eliminamos batch_size

    # Multiplicamos cada canal por su importancia y sumamos
    weighted_act = np.sum(conv_act * channel_importance, axis=-1)  # Shape: (32, 32, 20)
    group_heatmaps[diagnosis] += weighted_act

# Promediamos
for diag in group_heatmaps:
    group_heatmaps[diag] /= np.sum(y_train == diag)

def upsample_heatmap(heatmap_3d, target_shape=(64, 64, 40)):
    zoom_factors = (
        target_shape[0] / heatmap_3d.shape[0],
        target_shape[1] / heatmap_3d.shape[1],
        target_shape[2] / heatmap_3d.shape[2]
    )
    return zoom(heatmap_3d, zoom_factors)

# Upsample para cada grupo
upsampled_heatmaps = {
    diag: upsample_heatmap(heatmap)
    for diag, heatmap in group_heatmaps.items()
}

#!pip install nilearn

from nilearn import plotting
from nilearn.image import new_img_like

# Asumimos que X_train[0] es una imagen nibabel con affine correcto
template_img = nib.Nifti1Image(X_train[0,...,0], affine=np.eye(4))

for diag, diag_name in zip([0, 1, 2], ['AD', 'CN', 'FTD']):
    heatmap_img = new_img_like(template_img, upsampled_heatmaps[diag])

    plotting.plot_stat_map(
        heatmap_img,
        bg_img=template_img,  # Fondo anat√≥mico
        title=f'Mapa de Calor - {diag_name}',
        display_mode='ortho',
        cut_coords=(0, 0, 0),  # Centro del cerebro
        cmap='magma',
        threshold=np.percentile(upsampled_heatmaps[diag], 95)  # Top 5% de activaci√≥n
    )
plt.show()

import plotly.graph_objects as go

# Crear malla 3D del heatmap (ejemplo para AD)
x, y, z = np.mgrid[:64, :64, :40]
heatmap_ad = upsampled_heatmaps[0]

fig = go.Figure(data=go.Volume(
    x=x.flatten(),
    y=y.flatten(),
    z=z.flatten(),
    value=heatmap_ad.flatten(),
    isomin=np.percentile(heatmap_ad, 95),
    isomax=heatmap_ad.max(),
    opacity=0.1,
    surface_count=20,
    colorscale='hot'
))
fig.show()

import plotly.graph_objects as go
import numpy as np

# Volumen base: imagen T1 promedio o el primer sujeto (normalizado)
brain_volume = X_train[0, ..., 0]  # Shape: (64, 64, 40)
brain_volume = (brain_volume - brain_volume.min()) / (brain_volume.max() - brain_volume.min())  # Normalizamos

# Volumen de activaci√≥n para AD
heatmap_ad = upsampled_heatmaps[0]

# Coordenadas para ambos
x, y, z = np.mgrid[:64, :64, :40]

fig = go.Figure()

# üîµ Capa 1: cerebro base (gris√°ceo o azul)
fig.add_trace(go.Volume(
    x=x.flatten(),
    y=y.flatten(),
    z=z.flatten(),
    value=brain_volume.flatten(),
    isomin=0.1,
    isomax=1.0,
    opacity=0.05,
    surface_count=15,
    colorscale='Blues',
    showscale=False
))

# üî¥ Capa 2: activaci√≥n destacada (hot)
fig.add_trace(go.Volume(
    x=x.flatten(),
    y=y.flatten(),
    z=z.flatten(),
    value=heatmap_ad.flatten(),
    isomin=np.percentile(heatmap_ad, 95),
    isomax=heatmap_ad.max(),
    opacity=0.25,
    surface_count=8,
    colorscale='hot',
    showscale=True
))

fig.update_layout(
    title='Volumen cerebral con activaciones destacadas (Grad-CAM - AD)',
    scene=dict(
        xaxis_title='X',
        yaxis_title='Y',
        zaxis_title='Z',
        bgcolor='black'
    )
)

fig.show()

import plotly.graph_objects as go
import numpy as np

# Volumen base: imagen T1 promedio o el primer sujeto (normalizado)
brain_volume = X_train[0, ..., 0]  # Shape: (64, 64, 40)
brain_volume = (brain_volume - brain_volume.min()) / (brain_volume.max() - brain_volume.min())  # Normalizamos

# Volumen de activaci√≥n para CN
heatmap_ad = upsampled_heatmaps[1]

# Coordenadas para ambos
x, y, z = np.mgrid[:64, :64, :40]

fig = go.Figure()

# üîµ Capa 1: cerebro base (gris√°ceo o azul)
fig.add_trace(go.Volume(
    x=x.flatten(),
    y=y.flatten(),
    z=z.flatten(),
    value=brain_volume.flatten(),
    isomin=0.1,
    isomax=1.0,
    opacity=0.05,
    surface_count=15,
    colorscale='Blues',
    showscale=False
))

# üî¥ Capa 2: activaci√≥n destacada (hot)
fig.add_trace(go.Volume(
    x=x.flatten(),
    y=y.flatten(),
    z=z.flatten(),
    value=heatmap_ad.flatten(),
    isomin=np.percentile(heatmap_ad, 95),
    isomax=heatmap_ad.max(),
    opacity=0.25,
    surface_count=8,
    colorscale='hot',
    showscale=True
))

fig.update_layout(
    title='Volumen cerebral con activaciones destacadas (Grad-CAM - CN)',
    scene=dict(
        xaxis_title='X',
        yaxis_title='Y',
        zaxis_title='Z',
        bgcolor='black'
    )
)

fig.show()

import plotly.graph_objects as go
import numpy as np

# Volumen base: imagen T1 promedio o el primer sujeto (normalizado)
brain_volume = X_train[0, ..., 0]  # Shape: (64, 64, 40)
brain_volume = (brain_volume - brain_volume.min()) / (brain_volume.max() - brain_volume.min())  # Normalizamos

# Volumen de activaci√≥n para FTD
heatmap_ad = upsampled_heatmaps[2]

# Coordenadas para ambos
x, y, z = np.mgrid[:64, :64, :40]

fig = go.Figure()

# üîµ Capa 1: cerebro base (gris√°ceo o azul)
fig.add_trace(go.Volume(
    x=x.flatten(),
    y=y.flatten(),
    z=z.flatten(),
    value=brain_volume.flatten(),
    isomin=0.1,
    isomax=1.0,
    opacity=0.05,
    surface_count=15,
    colorscale='Blues',
    showscale=False
))

# üî¥ Capa 2: activaci√≥n destacada (hot)
fig.add_trace(go.Volume(
    x=x.flatten(),
    y=y.flatten(),
    z=z.flatten(),
    value=heatmap_ad.flatten(),
    isomin=np.percentile(heatmap_ad, 95),
    isomax=heatmap_ad.max(),
    opacity=0.25,
    surface_count=8,
    colorscale='hot',
    showscale=True
))

fig.update_layout(
    title='Volumen cerebral con activaciones destacadas (Grad-CAM - FTD)',
    scene=dict(
        xaxis_title='X',
        yaxis_title='Y',
        zaxis_title='Z',
        bgcolor='black'
    )
)

fig.show()

import matplotlib.pyplot as plt
import numpy as np

# Volumen anat√≥mico base (T1 de un paciente)
brain_img = X_train[0, :, :, :, 0]  # (64, 64, 40)

# Mapa de calor correspondiente (por ejemplo, AD)
heatmap = upsampled_heatmaps[0]     # (64, 64, 40)

# Corte axial central
z = brain_img.shape[2] // 2
base_slice = brain_img[:, :, z]
heatmap_slice = heatmap[:, :, z]

# Normalizamos para mejorar contraste
base_slice_norm = (base_slice - base_slice.min()) / (np.ptp(base_slice))

# Visualizaci√≥n
plt.figure(figsize=(6, 6))
plt.imshow(base_slice_norm, cmap='gray', origin='lower')
plt.imshow(heatmap_slice, cmap='hot', alpha=0.5, origin='lower')
plt.title("Corte Axial con Zonas de Activaci√≥n - Clase AD (0)")
plt.axis('off')
plt.colorbar(label="Activaci√≥n Grad-CAM")
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Volumen anat√≥mico base (T1 de un paciente)
brain_img = X_train[0, :, :, :, 0]  # (64, 64, 40)

# Mapa de calor correspondiente (por ejemplo, CN)
heatmap = upsampled_heatmaps[1]     # (64, 64, 40)

# Corte axial central
z = brain_img.shape[2] // 2
base_slice = brain_img[:, :, z]
heatmap_slice = heatmap[:, :, z]

# Normalizamos para mejorar contraste
base_slice_norm = (base_slice - base_slice.min()) / (np.ptp(base_slice))

# Visualizaci√≥n
plt.figure(figsize=(6, 6))
plt.imshow(base_slice_norm, cmap='gray', origin='lower')
plt.imshow(heatmap_slice, cmap='hot', alpha=0.5, origin='lower')
plt.title("Corte Axial con Zonas de Activaci√≥n - Clase CN (1)")
plt.axis('off')
plt.colorbar(label="Activaci√≥n Grad-CAM")
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Volumen anat√≥mico base (T1 de un paciente)
brain_img = X_train[0, :, :, :, 0]  # (64, 64, 40)

# Mapa de calor correspondiente (por ejemplo, FTD)
heatmap = upsampled_heatmaps[2]     # (64, 64, 40)

# Corte axial central
z = brain_img.shape[2] // 2
base_slice = brain_img[:, :, z]
heatmap_slice = heatmap[:, :, z]

# Normalizamos para mejorar contraste
base_slice_norm = (base_slice - base_slice.min()) / (np.ptp(base_slice))

# Visualizaci√≥n
plt.figure(figsize=(6, 6))
plt.imshow(base_slice_norm, cmap='gray', origin='lower')
plt.imshow(heatmap_slice, cmap='hot', alpha=0.5, origin='lower')
plt.title("Corte Axial con Zonas de Activaci√≥n - Clase FTD (2)")
plt.axis('off')
plt.colorbar(label="Activaci√≥n Grad-CAM")
plt.show()

"""Los mapas de calor representan los cerebros de pacientes con distintos perfiles observados con una vista axial, es decir, superior. La interpretaci√≥n de las im√°genes resultantes con posibles biomarcadores es la siguiente:

**1. Clase 1 -- CN (Control Normal):**
- La activaci√≥n es leve y dispersa, con peque√±os focos en √°reas perif√©ricas, particularmente en el cerebelo y la corteza occipital.
- Este patr√≥n indica que el modelo no detecta se√±ales patol√≥gicas evidentes: se basa en la falta de atrofia o cambios estructurales significativos, lo que concuerda con un perfil saludable.
- Se trata de un patr√≥n de activaci√≥n m√°s "neutral" y universal

**2. Clase 0 -- AD (Alzheimer):**
- La activaci√≥n se enfoca en √°reas m√°s internas y bilaterales, particularmente en:
  - L√≥bulos temporales mediales (regi√≥n del hipocampo).
  - Parietal inferior y subcorticales.
- Estas √°reas son biomarcadores tradicionales en AD: el modelo ha logrado identificarlas como rasgos del deterioro relacionado con el Alzheimer.
- Activaci√≥n intensa y localizada, lo que sugiere una se√±al anat√≥mica robusta

**3. Clase 2 -- FTD (Frontotemporal Dementia):**
- El patr√≥n es bastante parecido al de AD, pero se puede distinguir por:
  - Mayor influencia en √°reas frontales o laterales prefrontales.
  - Regiones m√°s elevadas, lo que coincide con la atrofia frontotemporal caracter√≠stica de FTD.
- La activaci√≥n es igualmente clara y precisa, aunque est√° m√°s alejada en comparaci√≥n con la de AD.

| Paciente | Clase real | Zonas activadas      | ¬øCoincide con otros de su clase? |
| -------- | ---------- | -------------------- | ------------------------------- |
| ...        | CN         | Occipital + cerebelo | Parcialmente                    |
| ...      | AD         | Temporal medial      | S√≠                              |
| ...      | FTD        | Frontal lateral      | Parcialmente, sobre todo frontal                              |
"""

import joblib

# Guardado del modelo
joblib.dump(rf_model, "/content/drive/MyDrive/TFG/rf_model.pkl")

print("‚úÖ Modelo RF guardado como 'rf_model.pkl'")

import numpy as np

# Guardamos embeddings e IDs de entrenamiento
np.savez_compressed(
    "/content/drive/MyDrive/TFG/embeddings_cnn2_train.npz",
    embeddings=embeddings_cnn2_train,
    ids=train_ids
)

# Guardamos embeddings e IDs de test
np.savez_compressed(
    "/content/drive/MyDrive/TFG/embeddings_cnn2_test.npz",
    embeddings=embeddings_cnn2_test,
    ids=test_ids
)

print("¬°Embeddings guardados correctamente!")
